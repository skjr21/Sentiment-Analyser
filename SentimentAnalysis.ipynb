{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e97afe",
   "metadata": {},
   "source": [
    "## Import Relevant Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7d2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cdd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset from kaggle: kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0fcf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63120fce",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6821c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c62072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "137d30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postive  25000\n",
      "negative  25000\n"
     ]
    }
   ],
   "source": [
    "#Checking the sentiment count\n",
    "\n",
    "print(\"postive \",np.sum(df[\"sentiment\"]==\"positive\"))\n",
    "print(\"negative \",np.sum(df[\"sentiment\"]==\"negative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482298d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thus we see the dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251e2e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically theres a family where a little boy J...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Matteis Love in the Time of Money is a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production The filming tech...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically theres a family where a little boy J...  negative\n",
       "4  Petter Matteis Love in the Time of Money is a ...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clear_noise(text):\n",
    "  \n",
    "  # removing html scripts using Beautiful Soup\n",
    "  text1=BeautifulSoup(text)\n",
    "  text1=text1.get_text()\n",
    "\n",
    "  #removing all other character than alphabets and numbers using regular expression\n",
    "  text2=re.sub('[^a-zA-Z0-9\\s]', '', text1)\n",
    "\n",
    "  return text2\n",
    "\n",
    "df[\"review\"]=df[\"review\"].apply(clear_noise)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc301a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All html and other special characters is removed now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14f543",
   "metadata": {},
   "source": [
    "## Preprocessing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a6c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now preprocessing the data according to the bag of word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1e32327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenisation (Breaking down the document into words)\n",
    "\n",
    "def tokenize(text):\n",
    "  return text.split()\n",
    "\n",
    "# 2. Stopword Removal (Removal of words which are not meaningful for the model)\n",
    "\n",
    "sw=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def stopword_removal(text):\n",
    "  useful_words=[w for w in text if w not in sw]\n",
    "  return useful_words\n",
    "\n",
    "# 3. Lemmatization (Changing all forms of a verb to root form like plays ,played etc. to play)\n",
    "\n",
    "wn=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "  words=[wn.lemmatize(w) for w in text]\n",
    "  return words\n",
    "\n",
    "# 4. Building a vocabulary (Each sentence will have a feature vector)\n",
    "\n",
    "def myTokenizer(text):\n",
    "  text1=tokenize(text.lower())\n",
    "  text1=stopword_removal(text1)\n",
    "  text1=lemmatize(text1)\n",
    "  return text1\n",
    "\n",
    "cv=CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a517c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do', 'only', 'those', \"haven't\", 'whom', 'who', \"it's\", 'through', 'her', 'where', 'other', \"isn't\", 'mustn', 'as', 'such', 'with', 'so', 'into', 'd', 'about', 'being', 'of', 'did', \"shouldn't\", 'hadn', 'an', 'further', 'does', \"won't\", 'doing', 'down', \"mustn't\", 'had', 'not', 'again', 'it', 'that', \"hadn't\", 'hers', 'i', \"she's\", 'them', 'yourselves', 'from', 'itself', 'off', 'between', 'ain', 'while', 'o', 'to', 'herself', 'we', 'when', 'its', 'were', 'they', 'this', 'haven', 'he', 'my', 'there', 'didn', 'needn', 'which', 'their', 'out', 'all', 'me', 'am', 'theirs', 'what', \"weren't\", 'up', 'aren', 'isn', 'any', 'is', \"didn't\", 'should', 'are', 'why', \"shan't\", 'has', 'because', 'yours', 'here', 'if', 'how', 'after', 'before', 're', \"you'd\", 'll', 'by', \"you'll\", 'now', 'hasn', 'until', 'own', 'him', 'themselves', 'nor', 've', 'in', \"aren't\", \"you're\", 'more', 'be', 'shan', 'she', 'each', 'too', 'over', \"don't\", 'or', 'no', 'just', 'have', 'few', 'been', 'ours', 'was', 'then', 'shouldn', 'below', 'but', 'weren', 'above', \"couldn't\", 'yourself', 'myself', 'during', 'a', 'wouldn', \"doesn't\", 'your', \"should've\", 'at', \"needn't\", 'and', \"wouldn't\", 'same', 'ma', 'don', 'both', 'won', 'you', 'will', 'than', 'wasn', 'himself', 'most', 'doesn', 'some', 's', 'm', \"wasn't\", 'very', \"that'll\", 'y', 'ourselves', 'can', 't', 'against', \"you've\", 'his', 'these', 'the', 'mightn', 'for', 'once', 'our', 'under', 'on', \"hasn't\", 'having', 'couldn', \"mightn't\"}\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09419c",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211b572",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c6cbe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) <class 'pandas.core.series.Series'>\n",
      "(10000,) <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "train_x=df.review[:40000]\n",
    "train_y=df.sentiment[:40000]\n",
    "\n",
    "test_x=df.review[40000:]\n",
    "test_y=df.sentiment[40000:]\n",
    "\n",
    "print(train_x.shape,type(train_x))\n",
    "print(test_x.shape,type(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62b10170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 181861)\n"
     ]
    }
   ],
   "source": [
    "vectorised_data=cv.fit_transform(train_x)\n",
    "print(vectorised_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144cc014",
   "metadata": {},
   "source": [
    "### Converting the test and train data in vectorised form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f167e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train=cv.transform(train_x)\n",
    "cv_test=cv.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65a1fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 181861)\n"
     ]
    }
   ],
   "source": [
    "print(cv_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9fc05f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Labelling the sentiment data\n",
    "\n",
    "lb=LabelBinarizer()\n",
    "sentiment_data=lb.fit_transform(df['sentiment'])\n",
    "print(sentiment_data.shape)\n",
    "\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd641d",
   "metadata": {},
   "source": [
    "### Building the MLP (Multi layer perceptron) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a6a85cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 16)                2909792   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,910,081\n",
      "Trainable params: 2,910,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(16,activation=\"relu\",input_shape=(181861,)))\n",
    "model.add(Dense(16,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer='sgd',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "948596fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.5857 - accuracy: 0.8024\n",
      "Epoch 2/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.5283 - accuracy: 0.8247\n",
      "Epoch 3/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.4826 - accuracy: 0.8363\n",
      "Epoch 4/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.4461 - accuracy: 0.8454\n",
      "Epoch 5/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.4165 - accuracy: 0.8528\n",
      "Epoch 6/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.3926 - accuracy: 0.8587\n",
      "Epoch 7/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.3727 - accuracy: 0.8649\n",
      "Epoch 8/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.3560 - accuracy: 0.8696\n",
      "Epoch 9/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.3415 - accuracy: 0.8749\n",
      "Epoch 10/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.3290 - accuracy: 0.8784\n",
      "Epoch 11/60\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.3178 - accuracy: 0.8825\n",
      "Epoch 12/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.3079 - accuracy: 0.8858\n",
      "Epoch 13/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.2989 - accuracy: 0.8898\n",
      "Epoch 14/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2907 - accuracy: 0.8923\n",
      "Epoch 15/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2833 - accuracy: 0.8952\n",
      "Epoch 16/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2764 - accuracy: 0.8974\n",
      "Epoch 17/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2702 - accuracy: 0.8993\n",
      "Epoch 18/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.2643 - accuracy: 0.9021\n",
      "Epoch 19/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2589 - accuracy: 0.9037\n",
      "Epoch 20/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2536 - accuracy: 0.9050\n",
      "Epoch 21/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2488 - accuracy: 0.9075\n",
      "Epoch 22/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2441 - accuracy: 0.9088\n",
      "Epoch 23/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2398 - accuracy: 0.9105\n",
      "Epoch 24/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.2356 - accuracy: 0.9118\n",
      "Epoch 25/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2316 - accuracy: 0.9143\n",
      "Epoch 26/60\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.2278 - accuracy: 0.9153\n",
      "Epoch 27/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2240 - accuracy: 0.9168\n",
      "Epoch 28/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.2204 - accuracy: 0.9184\n",
      "Epoch 29/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2171 - accuracy: 0.9197\n",
      "Epoch 30/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.2137 - accuracy: 0.9213\n",
      "Epoch 31/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.2105 - accuracy: 0.9226\n",
      "Epoch 32/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.2074 - accuracy: 0.9243\n",
      "Epoch 33/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.2041 - accuracy: 0.9260\n",
      "Epoch 34/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.2014 - accuracy: 0.9264\n",
      "Epoch 35/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1983 - accuracy: 0.9283\n",
      "Epoch 36/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1954 - accuracy: 0.9296\n",
      "Epoch 37/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.1927 - accuracy: 0.9314\n",
      "Epoch 38/60\n",
      "79/79 [==============================] - 1s 15ms/step - loss: 0.1900 - accuracy: 0.9325\n",
      "Epoch 39/60\n",
      "79/79 [==============================] - 1s 16ms/step - loss: 0.1873 - accuracy: 0.9330\n",
      "Epoch 40/60\n",
      "79/79 [==============================] - 1s 15ms/step - loss: 0.1848 - accuracy: 0.9344\n",
      "Epoch 41/60\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.1820 - accuracy: 0.9352\n",
      "Epoch 42/60\n",
      "79/79 [==============================] - 2s 22ms/step - loss: 0.1796 - accuracy: 0.9363\n",
      "Epoch 43/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1772 - accuracy: 0.9381\n",
      "Epoch 44/60\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.1744 - accuracy: 0.9385\n",
      "Epoch 45/60\n",
      "79/79 [==============================] - 1s 19ms/step - loss: 0.1723 - accuracy: 0.9398\n",
      "Epoch 46/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1699 - accuracy: 0.9405\n",
      "Epoch 47/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1675 - accuracy: 0.9418\n",
      "Epoch 48/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1654 - accuracy: 0.9426\n",
      "Epoch 49/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1632 - accuracy: 0.9434\n",
      "Epoch 50/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1606 - accuracy: 0.9452\n",
      "Epoch 51/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1584 - accuracy: 0.9459\n",
      "Epoch 52/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1563 - accuracy: 0.9474\n",
      "Epoch 53/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1545 - accuracy: 0.9478\n",
      "Epoch 54/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1523 - accuracy: 0.9490\n",
      "Epoch 55/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1502 - accuracy: 0.9497\n",
      "Epoch 56/60\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.1481 - accuracy: 0.9506\n",
      "Epoch 57/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1459 - accuracy: 0.9517\n",
      "Epoch 58/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1441 - accuracy: 0.9527\n",
      "Epoch 59/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1419 - accuracy: 0.9537\n",
      "Epoch 60/60\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.1401 - accuracy: 0.9543\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(cv_train,train_sentiments,epochs=60,batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927a6f8",
   "metadata": {},
   "source": [
    "## Accuracy of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "66c1e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2925 - accuracy: 0.8893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2924994230270386, 0.8892999887466431]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cv_test,test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "433ca443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model works with an accuracy of around 89 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b49e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
